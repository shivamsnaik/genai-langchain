{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2953be2b-53dd-4a05-8c70-89def489b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from constants import gemini_api_key, tavily_api_key\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeaa6e-3551-433e-a8e0-6790c03c8bb2",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "Providing context to the LLM models to better understand the question, hence allowing LLMs to better answer the questions.\n",
    "\n",
    "The RAG happens by passing the question to a Vector Database and performing similarity comparison between the embeddings of the question phrase and the documents present in the vector database. \n",
    "\n",
    "The most similar ones are fetched and fed into the model (as context for the model) along with the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cd06b-dfb7-49c6-9de6-1c8ffe33bd1f",
   "metadata": {},
   "source": [
    "## Testing the embedding model with test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ade1b5-0ad7-49db-9e79-9399621ddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(google_api_key = gemini_api_key, model=\"gemini-pro\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(google_api_key = gemini_api_key, model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9d6259-2487-42b6-8f5c-0180658e698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Today is a sunny day\", \n",
    "    \"Today is april fools day\", \n",
    "    \"Today is a snowy day\", \n",
    "    \"Robert Downey is the Iron Man\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dce0df5-8653-4375-87a4-35a1439f8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the embedding vector for the following queries\n",
    "vectors = [embeddings.embed_query(query) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0909e2-10ab-46ac-803b-814e5f97f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5faac8d-3dd2-438a-a9b5-7cf1cff35af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st and 2nd query:  [[0.87064364]]\n",
      "1st and 3nd query:  [[0.93163611]]\n",
      "2nd and 3rd query:  [[0.87995205]]\n",
      "1st and 4th query:  [[0.77332271]]\n",
      "2nd and 4th query:  [[0.80163847]]\n",
      "3rd and 4th query:  [[0.7808282]]\n"
     ]
    }
   ],
   "source": [
    "# Verifying the similarities between these documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"1st and 2nd query: \", cosine_similarity([vectors[0]], [vectors[1]]))\n",
    "print(\"1st and 3nd query: \", cosine_similarity([vectors[0]], [vectors[2]]))\n",
    "print(\"2nd and 3rd query: \", cosine_similarity([vectors[1]], [vectors[2]]))\n",
    "print(\"1st and 4th query: \", cosine_similarity([vectors[0]], [vectors[3]]))\n",
    "print(\"2nd and 4th query: \", cosine_similarity([vectors[1]], [vectors[3]]))\n",
    "print(\"3rd and 4th query: \", cosine_similarity([vectors[2]], [vectors[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bbd97-839a-4cd3-aeed-0dba3e3eace6",
   "metadata": {},
   "source": [
    "## Using GoogleEmbeddings in Langchain to build RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bb34a-ef6c-4c05-8246-918985d64ef1",
   "metadata": {},
   "source": [
    "### 1. Testing with in memory vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc6d64d-8f75-45e8-96ea-897768019834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install docarray\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca841f5d-131c-48b5-a395-ead1200b8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Tony Stark was kidnapped by thugs for making a lethal weapon in a cave\",\n",
    "        \"Tony Stark designed a miniature arc reactor in a cave\"\n",
    "    ],\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a938afff-11ee-4048-b56b-8bcb6ba39971",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4365c51d-1ff5-45f6-b53a-adc2134c5075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not specify why Tony Stark was kidnapped.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the questions based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"Why was Tony Stark kidnapped?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff53901d-de58-4f63-bf5b-3dbfd89377a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The provided context does not mention who Tony Stark is.', response_metadata={'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(setup_and_retrieval | prompt | model).invoke(\"Who is Tony Stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf598d-34e1-4593-892e-1fa11fdc2bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
