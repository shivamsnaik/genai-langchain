{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2953be2b-53dd-4a05-8c70-89def489b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from constants import gemini_api_key, tavily_api_key\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeaa6e-3551-433e-a8e0-6790c03c8bb2",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "Providing context to the LLM models to better understand the question, hence allowing LLMs to better answer the questions.\n",
    "\n",
    "The RAG happens by passing the question to a Vector Database and performing similarity comparison between the embeddings of the question phrase and the documents present in the vector database. \n",
    "\n",
    "The most similar ones are fetched and fed into the model (as context for the model) along with the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cd06b-dfb7-49c6-9de6-1c8ffe33bd1f",
   "metadata": {},
   "source": [
    "## Testing the embedding model with test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95ade1b5-0ad7-49db-9e79-9399621ddfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(google_api_key = gemini_api_key, model=\"gemini-pro\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(google_api_key = gemini_api_key, model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e9d6259-2487-42b6-8f5c-0180658e698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Today is a sunny day\", \n",
    "    \"Today is april fools day\", \n",
    "    \"Today is a snowy day\", \n",
    "    \"Robert Downey is the Iron Man\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dce0df5-8653-4375-87a4-35a1439f8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the embedding vector for the following queries\n",
    "vectors = [embeddings.embed_query(query) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c0909e2-10ab-46ac-803b-814e5f97f669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5faac8d-3dd2-438a-a9b5-7cf1cff35af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st and 2nd query:  [[0.87064364]]\n",
      "1st and 3nd query:  [[0.93163611]]\n",
      "2nd and 3rd query:  [[0.87995205]]\n",
      "1st and 4th query:  [[0.77332271]]\n",
      "2nd and 4th query:  [[0.80163847]]\n",
      "3rd and 4th query:  [[0.7808282]]\n"
     ]
    }
   ],
   "source": [
    "# Verifying the similarities between these documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"1st and 2nd query: \", cosine_similarity([vectors[0]], [vectors[1]]))\n",
    "print(\"1st and 3nd query: \", cosine_similarity([vectors[0]], [vectors[2]]))\n",
    "print(\"2nd and 3rd query: \", cosine_similarity([vectors[1]], [vectors[2]]))\n",
    "print(\"1st and 4th query: \", cosine_similarity([vectors[0]], [vectors[3]]))\n",
    "print(\"2nd and 4th query: \", cosine_similarity([vectors[1]], [vectors[3]]))\n",
    "print(\"3rd and 4th query: \", cosine_similarity([vectors[2]], [vectors[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44bbd97-839a-4cd3-aeed-0dba3e3eace6",
   "metadata": {},
   "source": [
    "## Using GoogleEmbeddings in Langchain to build RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bb34a-ef6c-4c05-8246-918985d64ef1",
   "metadata": {},
   "source": [
    "### 1. Testing with in memory vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffc6d64d-8f75-45e8-96ea-897768019834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install docarray\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "124ca78f-b818-4d6f-b8b5-d9e5f1ecab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates embedding vectors for all the queries and the documents provided for the vector DB\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key = gemini_api_key, \n",
    "    task_type=\"retrieval_document\", \n",
    "    model = \"models/embedding-001\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca841f5d-131c-48b5-a395-ead1200b8144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(model='models/embedding-001', task_type='retrieval_document', google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stores the documents in the vector DB and utilized Google Model Embeddings for similarity estimation\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Tony Stark is a rich philanthropist and a playboy\",\n",
    "        \"Tony Stark was kidnapped by thugs for making a lethal weapon in a cave\",\n",
    "        \"Tony Stark designed a miniature arc reactor in a cave\"\n",
    "    ],\n",
    "    embedding = embeddings\n",
    ")\n",
    "vectorstore.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a938afff-11ee-4048-b56b-8bcb6ba39971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves relevant documents from the store based on a given query/question\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352764c-071d-49a6-a493-1ccaba49f01a",
   "metadata": {},
   "source": [
    "The flow is as follows:\n",
    "1. The user enters a question for the model.  \n",
    "2. **setup_and_retrieval** follows the prompt template to pass the context and question. However, two operations run parallelly.\n",
    "    - Firstly, the question is passed to the **retriever** and the relavant documents are fetched in the form of **context**.\n",
    "    - Simultaneously, the question is passed to the **question** key by the Runnablepassthrough.\n",
    "3. The received context and question are passed to prompttemplate object to create a **PromptValue**.\n",
    "4. Model takes the **PromptValue**, processes it, and returns the response in the form of **ChatMessage**.\n",
    "5. The **ChatMessage** is passed to the string parser to finally display the models output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4365c51d-1ff5-45f6-b53a-adc2134c5075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tony Stark was kidnapped by thugs because he had designed a lethal weapon in a cave, making him a target for those who sought to use his technology for their own gain.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Use the context provided:\n",
    "{context} and creatively answer the following question: {question}\n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    google_api_key = gemini_api_key, \n",
    "    model=\"gemini-pro\",\n",
    "    temperature=1,\n",
    "    top_p = 1\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"Why was tony stark kidnapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf598d-34e1-4593-892e-1fa11fdc2bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
