{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f073e9-f222-44bc-9059-c9ed65a229ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from constants import gemini_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da84ccc-e579-4427-9f10-3efa0e1dcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(google_api_key=gemini_api_key, model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411aa064-be90-4f9e-a338-3c422dd7adeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What do you call an ice cream that can't make up its mind?\\n\\nA flip-flop!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# The pipe is used to chain multiple components together in a sequencial fashion\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e59cb1-bb13-422d-97e9-d20a3053383b",
   "metadata": {},
   "source": [
    "## Understanding components under to Hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2812e72-915e-4a17-98c6-0aed7417dd04",
   "metadata": {},
   "source": [
    "### 1. Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e9db1c-3f68-4394-a5a4-745bb63de9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='Tell me a short joke about {topic}'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The template behind the scenes\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c390e1-de78-4b7b-8f38-25d7855afdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We receive the message in the form of Human Message\n",
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c92c3a-216c-4cf8-9fc5-24f46939168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad56d36-1472-446d-81d3-db3f7897242a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba94ee2-656a-4ccc-bf48-6c60d3c23aa1",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5307186-8ebe-4c3d-98ab-5c6528272006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the ice cream truck get lost?\\n\\nBecause it didn't have a cone-pass!\", response_metadata={'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The raw response contain the content as well as details such as safety settings\n",
    "# Note: As we have initialized a ChatModel, response is in the form of BaseMessage (whose subclass is AIMessage)\n",
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab0a7988-b226-4dff-9d81-e6a29574c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of an LLM object, the response is in the form of string\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "llm = GoogleGenerativeAI(google_api_key = gemini_api_key, model = \"gemini-pro\")\n",
    "llm_response = llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d5a46-ad96-49a4-a572-95234bd21f7a",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f5acc-6518-4de6-8a37-35331d6366a7",
   "metadata": {},
   "source": [
    "#### Both will parse the string and give a formatted string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3711535b-fd4c-497c-a12c-331cea90e192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream truck get lost?\\n\\nBecause it didn't have a cone-pass!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f716c43b-4d41-42e9-a5d7-3de370b5baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What do you call an ice cream that's always singing?\\n\\nA cone-cert!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
